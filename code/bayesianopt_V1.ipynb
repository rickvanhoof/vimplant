{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Wed May 6 2021\\n\\n@authors: R. van Hoof & A. Lozano\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed May 6 2021\n",
    "\n",
    "@authors: R. van Hoof & A. Lozano\n",
    "\"\"\"\n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os.path\n",
    "import pickle # needed to store the results\n",
    "from copy import deepcopy\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import mutual_info_score as MI\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Integer, Categorical, Real\n",
    "from skopt.utils import cook_initial_point_generator\n",
    "from skopt import gp_minimize\n",
    "\n",
    "########################\n",
    "### Custom functions ###\n",
    "########################\n",
    "from ninimplant import pol2cart, get_xyz # matrix rotation/translation ect\n",
    "from lossfunc import DC, KL, get_yield, hellinger_distance\n",
    "from electphos import create_grid, reposition_grid, implant_grid, get_phosphenes, prf_to_phos, gen_dummy_phos, get_cortical_magnification, cortical_spread\n",
    "import visualsectors as gvs\n",
    "\n",
    "# ignore \"True-divide\" warning\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "## INIT ##\n",
    "##########\n",
    "\n",
    "# datafolder = '/path/to/data/'\n",
    "# outputfolder = '/path/to/data/'\n",
    "# datafolder = 'F:/Rick/Surfdrive_BACKUP/Data/NESTOR/HCP/subjects/'\n",
    "datafolder = r'C:/Users/Lozano/Desktop/NIN/bayesian_optimization_paper/data/subjects/'\n",
    "outputfolder =  r'C:/Users/Lozano/Desktop/NIN/bayesian_optimization_paper/data/output/'\n",
    "\n",
    "# determine range of parameters used in optimization\n",
    "dim1 = Integer(name='alpha', low=-90, high=90) # visual degrees \n",
    "dim2 = Integer(name='beta', low=-15, high=110) # visual degrees - -15 is probably lowest possible angle, otherwise other hem is in the way if hem = RH -> low = -110, high = 15\n",
    "dim3 = Integer(name='offset_from_base', low=0, high=40) # in mm\n",
    "dim4 = Integer(name='shank_length', low=10, high=40) # mm\n",
    "dimensions = [dim1, dim2, dim3, dim4]\n",
    "\n",
    "num_calls = 150\n",
    "x0 = (0,0,20,25) # initial values for the four dimensions\n",
    "num_initial_points = 10\n",
    "dc_percentile = 50\n",
    "n_contactpoints_shank = 10\n",
    "spacing_along_xy = 1\n",
    "WINDOWSIZE = 1000\n",
    "\n",
    "# lists of loss term combinations to loop through\n",
    "    # Dice Coefficient (1, 0, 0)\n",
    "    # Yield (0, 1, 0)\n",
    "    # Hellinger Distance (0, 0, 1)\n",
    "loss_comb = ([(1, 0.1, 1)]) # weights for loss terms\n",
    "loss_names = (['dice-yield-HD']) # substring in output filename\n",
    "\n",
    "# lists of target maps to loop through\n",
    "targ_comb = ([gvs.upper_sector(windowsize=WINDOWSIZE, fwhm=800, radiusLow=0, radiusHigh=500, plotting=False), \n",
    "              gvs.lower_sector(windowsize=WINDOWSIZE, fwhm=800, radiusLow=0, radiusHigh=500, plotting=False),\n",
    "              gvs.inner_ring(windowsize=WINDOWSIZE, fwhm=400, radiusLow=0, radiusHigh=250, plotting=False),\n",
    "              gvs.complete_gauss(windowsize=1000, fwhm=1200, radiusLow=0, radiusHigh=500, center=None, plotting=False)])\n",
    "targ_names = (['targ-upper', 'targ-lower', 'targ-inner', 'targ-full'])\n",
    "\n",
    "# constants pRF model\n",
    "cort_mag_model = 'wedge-dipole' # which cortex model to use for the cortical magnification\n",
    "view_angle = 90 #in degrees of visual angle\n",
    "amp = 100 #stimulation amplitude in micro-amp (higher stimulation -> more tissue activated)\n",
    "\n",
    "# INIT Bayes\n",
    "amax = 1\n",
    "bmax = 1\n",
    "cmax = 1000\n",
    "N=5\n",
    "delta=0.2\n",
    "thresh=0.05\n",
    "\n",
    "# subjects to include\n",
    "subj_list = [118225, 144226, 162935, 176542, 187345, 200614, 251833, 389357, 547046, 671855, 789373, 901139,  \n",
    "100610, 125525, 145834, 164131, 177140, 191033, 201515, 257845, 393247, 550439, 680957, 814649, 901442, \n",
    "102311, 126426, 146129, 164636, 177645, 191336, 203418, 263436, 395756, 552241, 690152, 818859, 905147, \n",
    "102816, 128935, 146432, 165436, 177746, 191841, 204521, 283543, 397760, 562345, 706040, 825048, 910241, \n",
    "104416, 130114, 146735, 167036, 178142, 192439, 205220, 318637, 401422, 572045, 724446, 826353, 926862, \n",
    "105923, 130518, 146937, 167440, 178243, 192641, 209228, 320826, 406836, 573249, 725751, 833249, 927359, \n",
    "108323, 131217, 148133, 169040, 178647, 193845, 212419, 330324, 412528, 581450, 732243, 859671, 942658, \n",
    "109123, 131722, 150423, 169343, 180533, 195041, 214019, 346137, 429040, 585256, 751550, 861456, 943862, \n",
    "111312, 132118, 155938, 169444, 181232, 196144, 214524, 352738, 436845, 601127, 757764, 871762, 951457, \n",
    "111514, 134627, 156334, 169747, 181636, 197348, 221319, 360030, 463040, 617748, 765864, 872764, 958976, \n",
    "114823, 134829, 157336, 171633, 182436, 198653, 233326, 365343, 467351, 627549, 770352, 878776, 966975, \n",
    "115017, 135124, 158035, 172130, 182739, 199655, 239136, 380036, 525541, 638049, 771354, 878877, 971160, \n",
    "115825, 137128, 158136, 173334, 185442, 200210, 246133, 381038, 536647, 644246, 782561, 898176, 973770, \n",
    "116726, 140117, 159239, 175237, 186949, 200311, 249947, 385046, 541943, 654552, 783462, 899885, 995174, 'fsaverage']\n",
    "\n",
    "subj_list = [100206]\n",
    "subj_list = [100610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "## Functions related to Bayesian optimization ##\n",
    "################################################\n",
    "\n",
    "def custom_stopper(res, N=5, delta=0.2, thresh=0.05):\n",
    "    '''\n",
    "    Returns True (stops the optimization) when \n",
    "    the difference between best and worst of the best N are below delta AND the best is below thresh\n",
    "    \n",
    "    N = last number of cost values to track\n",
    "    delta = ratio best and worst\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if len(res.func_vals) >= N:\n",
    "        func_vals = np.sort(res.func_vals)\n",
    "        worst = func_vals[N - 1]\n",
    "        best = func_vals[0]\n",
    "        \n",
    "        return (abs((best - worst)/worst) < delta) & (best < thresh)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def f(alpha, beta, offset_from_base, shank_length):\n",
    "    \"\"\"\n",
    "    This function encapsulates the electrode placement procedure and returns the cost value by \n",
    "    comparing the resulting phosphene map with the target map.    \n",
    "    * First it creats a grid based on the four parameters. \n",
    "    * Phosphenes are generated based on the grid's contact points, \n",
    "      and their sizes are determined using cortical magnification and spread values. \n",
    "    * These phosphenes are converted into a 2D image representation. \n",
    "      The function then computes the dice coefficient and yield, and calculates the Hellinger \n",
    "      distance between the generated image and a target density. \n",
    "    * The resulting cost is a combination of these factors, \n",
    "      with penalties applied if the grid is invalid. \n",
    "    * The function also handles cases of invalid values and prints diagnostic information. \n",
    "    * Ultimately, the function returns the calculated cost used by the bayesopt algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    penalty = 0.25\n",
    "    new_angle = (float(alpha), float(beta), 0)    \n",
    "    \n",
    "    # create grid\n",
    "    orig_grid = create_grid(start_location, shank_length, n_contactpoints_shank, spacing_along_xy, offset_from_origin=0)\n",
    "    \n",
    "    # implanting grid\n",
    "    _, contacts_xyz_moved, _, _, _, _, _, _, grid_valid = implant_grid(gm_mask, orig_grid, start_location, new_angle, offset_from_base)\n",
    "\n",
    "    # get angle, ecc and rfsize for contactpoints (phosphenes[0-2][:] 0 angle x 1 ecc x 2 rfsize)    \n",
    "    phosphenes = get_phosphenes(contacts_xyz_moved, good_coords, polar_map, ecc_map, sigma_map)\n",
    "    phosphenes_V1 = get_phosphenes(contacts_xyz_moved, good_coords_V1, polar_map, ecc_map, sigma_map)\n",
    "    phosphenes_V2 = get_phosphenes(contacts_xyz_moved, good_coords_V2, polar_map, ecc_map, sigma_map)   \n",
    "    phosphenes_V3 = get_phosphenes(contacts_xyz_moved, good_coords_V3, polar_map, ecc_map, sigma_map)\n",
    "    \n",
    "    #the inverse cortical magnification in degrees (visual angle)/mm tissue\n",
    "    M = 1 / get_cortical_magnification(phosphenes_V1[:,1], cort_mag_model) \n",
    "    spread = cortical_spread(amp) #radius of current spread in the tissue, in mm\n",
    "    sizes = spread*M #radius of current spread * cortical magnification = rf radius in degrees\n",
    "    sigmas = sizes / 2  # radius to sigma of gaussian\n",
    "    \n",
    "    # phosphene size based on CMF + stim amp\n",
    "    phosphenes_V1[:,2] = sigmas\n",
    "\n",
    "    # generate map using Gaussians\n",
    "    # transforming obtained phosphenes to a 2d image    \n",
    "    phospheneMap = np.zeros((WINDOWSIZE,WINDOWSIZE), 'float32') \n",
    "    phospheneMap = prf_to_phos(phospheneMap, phosphenes_V1, view_angle=view_angle, phSizeScale=1)    \n",
    "    phospheneMap /= phospheneMap.max()\n",
    "    phospheneMap /= phospheneMap.sum()\n",
    "\n",
    "    # bin_thesh determines size target\n",
    "    bin_thresh=np.percentile(target_density, dc_percentile)# np.min(target_density) \n",
    "\n",
    "    # compute dice coefficient -> should be 1 -> invert cost \n",
    "    dice, im1, im2 = DC(target_density, phospheneMap, bin_thresh)\n",
    "    par1 = 1.0 - (a * dice)\n",
    "\n",
    "    # compute yield -> should be 1 -> invert cost\n",
    "    grid_yield = get_yield(contacts_xyz_moved, good_coords)\n",
    "    par2 = 1.0 - (b * grid_yield)\n",
    "\n",
    "    # compute hellinger distance -> should be small -> keep cost\n",
    "    hell_d = hellinger_distance(phospheneMap.flatten(), target_density.flatten())    \n",
    "    \n",
    "    ## validations steps\n",
    "    if np.isnan(phospheneMap).any() or np.sum(phospheneMap) == 0:\n",
    "        par1 = 1\n",
    "        print('map is nan or 0')\n",
    "    \n",
    "    if np.isnan(hell_d) or np.isinf(hell_d):\n",
    "        par3 = 1\n",
    "        print('Hellington is nan or inf')\n",
    "    else:\n",
    "        par3 = c * hell_d\n",
    "    \n",
    "    # combine cost functions\n",
    "    cost = par1 + par2 + par3\n",
    "\n",
    "    # when some contact points are outside of the hemisphere (convex), add penalty\n",
    "    if not grid_valid:\n",
    "        cost = par1 + penalty + par2 + penalty + par3 + penalty\n",
    "    \n",
    "    # check if cost contains invalid value\n",
    "    if np.isnan(cost) or np.isinf(cost):\n",
    "        cost = 3\n",
    "    \n",
    "    print('    ', \"{:.2f}\".format(cost), \"{:.2f}\".format(dice), \"{:.2f}\".format(grid_yield), \"{:.2f}\".format(par3), grid_valid)\n",
    "    return cost\n",
    "\n",
    "def f_manual(alpha, beta, offset_from_base, shank_length, good_coords, good_coords_V1, good_coords_V2, good_coords_V3, target_density):\n",
    "    '''\n",
    "    Copy from f, to obtain phosphene map and contact points for the optimized parameters. Used to visualize results.\n",
    "\n",
    "    also returns coords used ect.\n",
    "    '''\n",
    "    \n",
    "    penalty = 0.25\n",
    "    new_angle = (float(alpha), float(beta), 0)\n",
    "    \n",
    "    # create grid\n",
    "    orig_grid = create_grid(start_location, shank_length, n_contactpoints_shank, spacing_along_xy, offset_from_origin=0)\n",
    "\n",
    "    # implanting grid\n",
    "    ref_contacts_xyz, contacts_xyz_moved, refline, refline_moved, projection, ref_orig, ray_visualize, new_location, grid_valid = implant_grid(gm_mask, orig_grid, start_location, new_angle, offset_from_base)\n",
    "\n",
    "    # get angle, ecc and rfsize for contactpoints in each ROI (phosphenes[0-2][:] 0 angle x 1 ecc x 2 rfsize)\n",
    "    phosphenes =    get_phosphenes(contacts_xyz_moved, good_coords, polar_map, ecc_map, sigma_map)\n",
    "    phosphenes_V1 = get_phosphenes(contacts_xyz_moved, good_coords_V1, polar_map, ecc_map, sigma_map)\n",
    "    phosphenes_V2 = get_phosphenes(contacts_xyz_moved, good_coords_V2, polar_map, ecc_map, sigma_map)\n",
    "    phosphenes_V3 = get_phosphenes(contacts_xyz_moved, good_coords_V3, polar_map, ecc_map, sigma_map)\n",
    "    \n",
    "    #the inverse cortical magnification in degrees (visual angle)/mm tissue\n",
    "    M = 1 / get_cortical_magnification(phosphenes_V1[:,1], cort_mag_model)\n",
    "    spread = cortical_spread(amp) #radius of current spread in the tissue, in mm\n",
    "    sizes = spread*M #radius of current spread * cortical magnification = rf radius in degrees\n",
    "    sigmas = sizes / 2  # radius to sigma of gaussian\n",
    "    \n",
    "    # phosphene size based on CMF + stim amp\n",
    "    phosphenes_V1[:,2] = sigmas\n",
    "\n",
    "    # generate map using Gaussians\n",
    "    # transforming obtained phosphenes to a 2d image\n",
    "    phospheneMap = np.zeros((WINDOWSIZE,WINDOWSIZE), 'float32')\n",
    "    phospheneMap = prf_to_phos(phospheneMap, phosphenes_V1, view_angle=view_angle, phSizeScale=1)\n",
    "    phospheneMap /= phospheneMap.max()\n",
    "    phospheneMap /= phospheneMap.sum()\n",
    "    print(view_angle)\n",
    "    \n",
    "    # can we relate bin_thesh to an eccentricity value? -> taken care of by masking the targets -> CHANGE TO 0.99999\n",
    "    bin_thresh=np.percentile(target_density, dc_percentile)# np.min(target_density) # bin_thesh determines size target\n",
    "\n",
    "    # compute dice coefficient -> should be large -> invert cost \n",
    "    dice, im1, im2 = DC(target_density, phospheneMap, bin_thresh)\n",
    "    par1 = 1.0 - (a * dice)\n",
    "\n",
    "    # compute yield -> should be 1 -> invert cost\n",
    "    grid_yield = get_yield(contacts_xyz_moved, good_coords)\n",
    "    par2 = 1.0 - (b * grid_yield)  \n",
    "    \n",
    "    # very important to normalize target density to same range as phospheneMap!\n",
    "    target_density /= target_density.max()\n",
    "    target_density /= target_density.sum()        \n",
    "    \n",
    "    # compute Hellinger distance -> should be small -> keep cost\n",
    "    hell_d = hellinger_distance(phospheneMap.flatten(), target_density.flatten())\n",
    "    \n",
    "    ## validations steps\n",
    "    if np.isnan(phospheneMap).any() or np.sum(phospheneMap) == 0:\n",
    "        par1 = 1\n",
    "        print('map is nan or 0')\n",
    "    \n",
    "    if np.isnan(hell_d) or np.isinf(hell_d):\n",
    "        par3 = 1\n",
    "        print('Hellington is nan or inf')\n",
    "    else:\n",
    "        par3 = c * hell_d\n",
    "    \n",
    "    # combine cost functions\n",
    "    cost = par1 + par2 + par3\n",
    "\n",
    "    # when some contact points are outside of the hemisphere (convex), add penalty\n",
    "    if not grid_valid:\n",
    "        cost = par1 + penalty + par2 + penalty + par3 + penalty\n",
    "    \n",
    "    # check if cost contains invalid value\n",
    "    if np.isnan(cost) or np.isinf(cost):\n",
    "        cost = 3\n",
    "    \n",
    "    return grid_valid, dice, hell_d, grid_yield, phosphenes, phosphenes_V1, phosphenes_V2, phosphenes_V3, contacts_xyz_moved, phospheneMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of subjects: 1\n",
      "target:  targ-upper\n",
      "loss:  dice-yield-HD\n",
      "a,b,c:  1 0.1 1\n",
      "     2.71 0.15 0.60 0.92 True\n",
      "     2.83 0.07 0.32 0.93 True\n",
      "     3.63 0.04 0.23 0.95 False\n",
      "     2.77 0.10 0.42 0.91 True\n",
      "     3.60 0.05 0.34 0.93 False\n",
      "     2.93 0.03 0.21 0.98 True\n",
      "     2.87 0.05 0.43 0.95 True\n",
      "     2.80 0.07 0.60 0.93 True\n",
      "     3.58 0.05 0.34 0.92 False\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "## Main Sim Loop ##\n",
    "###################\n",
    "\n",
    "# set file names\n",
    "fname_ang = 'inferred_angle.mgz'\n",
    "fname_ecc = 'inferred_eccen.mgz'\n",
    "fname_sigma = 'inferred_sigma.mgz'\n",
    "fname_anat = 'T1.mgz'\n",
    "fname_aparc = 'aparc+aseg.mgz'\n",
    "fname_label = 'inferred_varea.mgz'\n",
    "print('number of subjects: ' + str(len(subj_list)))\n",
    "\n",
    "# set beta angle constraints according to hemisphere\n",
    "dim2_lh = Integer(name='beta', low=-15, high=110)\n",
    "dim2_rh = Integer(name='beta', low=-110, high=15)\n",
    "\n",
    "# loop through phosphene target maps and combinations of loss terms\n",
    "for target_density, ftarget in zip(targ_comb, targ_names):\n",
    "    for (a, b, c), floss in zip(loss_comb, loss_names):\n",
    "        # set target\n",
    "        target_density /= target_density.max()\n",
    "        target_density /= target_density.sum()\n",
    "        # can we relate bin_thesh to an eccentricity value?\n",
    "        bin_thresh=np.percentile(target_density, dc_percentile ) #np.min(target_density) # bin_thesh determines size target\n",
    "        target_density_bin = (target_density > bin_thresh).astype(bool)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('Horizontally stacked subplots')\n",
    "        plt.subplot(1,2,1).imshow(target_density, cmap = 'seismic')\n",
    "        plt.subplot(1,2,2).imshow(target_density_bin, cmap = 'seismic')\n",
    "\n",
    "        for s in subj_list:\n",
    "            data_dir = datafolder + str(s)+ '/T1w/' + str(s) + '/mri/'            \n",
    "            if s == 'fsaverage':\n",
    "                data_dir = datafolder + str(s) + '/mri/'\n",
    "                \n",
    "            # load maps\n",
    "            ang_img = nib.load(data_dir+fname_ang)\n",
    "            polar_map = ang_img.get_fdata()\n",
    "            ecc_img = nib.load(data_dir+fname_ecc)\n",
    "            ecc_map = ecc_img.get_fdata()\n",
    "            sigma_img = nib.load(data_dir+fname_sigma)\n",
    "            sigma_map = sigma_img.get_fdata()                \n",
    "            aparc_img = nib.load(data_dir+fname_aparc)\n",
    "            aparc_roi = aparc_img.get_fdata()\n",
    "            label_img = nib.load(data_dir+fname_label)\n",
    "            label_map = label_img.get_fdata()\n",
    "\n",
    "            # compute valid voxels\n",
    "            dot = (ecc_map * polar_map)\n",
    "            good_coords = np.asarray(np.where(dot != 0.0))\n",
    "\n",
    "            # filter gm per hemisphere\n",
    "            cs_coords_rh = np.where(aparc_roi == 1021)\n",
    "            cs_coords_lh = np.where(aparc_roi == 2021)\n",
    "            gm_coords_rh = np.where((aparc_roi >= 1000) & (aparc_roi < 2000))\n",
    "            gm_coords_lh = np.where(aparc_roi > 2000)\n",
    "            xl,yl,zl = get_xyz(gm_coords_lh)\n",
    "            xr,yr,zr = get_xyz(gm_coords_rh)\n",
    "            GM_LH = np.array([xl,yl,zl]).T\n",
    "            GM_RH = np.array([xr,yr,zr]).T\n",
    "\n",
    "            # extract labels\n",
    "            V1_coords_rh = np.asarray(np.where(label_map == 1))\n",
    "            V1_coords_lh = np.asarray(np.where(label_map == 1))\n",
    "            V2_coords_rh = np.asarray(np.where(label_map == 2))\n",
    "            V2_coords_lh = np.asarray(np.where(label_map == 2))\n",
    "            V3_coords_rh = np.asarray(np.where(label_map == 3))\n",
    "            V3_coords_lh = np.asarray(np.where(label_map == 3))\n",
    "\n",
    "            # divide V1 coords per hemisphere\n",
    "            good_coords_lh = np.array([x for x in set(tuple(x) for x in np.round(good_coords).T) & set(tuple(x) for x in np.round(gm_coords_lh).T)]).T\n",
    "            good_coords_rh = np.array([x for x in set(tuple(x) for x in np.round(good_coords).T) & set(tuple(x) for x in np.round(gm_coords_rh).T)]).T\n",
    "            V1_coords_lh = np.array([x for x in set(tuple(x) for x in np.round(V1_coords_lh).T) & set(tuple(x) for x in np.round(gm_coords_lh).T)]).T\n",
    "            V1_coords_rh = np.array([x for x in set(tuple(x) for x in np.round(V1_coords_rh).T) & set(tuple(x) for x in np.round(gm_coords_rh).T)]).T\n",
    "            V2_coords_lh = np.array([x for x in set(tuple(x) for x in np.round(V2_coords_lh).T) & set(tuple(x) for x in np.round(gm_coords_lh).T)]).T\n",
    "            V2_coords_rh = np.array([x for x in set(tuple(x) for x in np.round(V2_coords_rh).T) & set(tuple(x) for x in np.round(gm_coords_rh).T)]).T\n",
    "            V3_coords_lh = np.array([x for x in set(tuple(x) for x in np.round(V3_coords_lh).T) & set(tuple(x) for x in np.round(gm_coords_lh).T)]).T\n",
    "            V3_coords_rh = np.array([x for x in set(tuple(x) for x in np.round(V3_coords_rh).T) & set(tuple(x) for x in np.round(gm_coords_rh).T)]).T           \n",
    "\n",
    "            # find center of left and right calcarine sulci\n",
    "            median_lh = [np.median(cs_coords_lh[0][:]), np.median(cs_coords_lh[1][:]), np.median(cs_coords_lh[2][:])]\n",
    "            median_rh = [np.median(cs_coords_rh[0][:]), np.median(cs_coords_rh[1][:]), np.median(cs_coords_rh[2][:])]\n",
    "\n",
    "            # get GM mask and compute dorsal/posterior planes\n",
    "            gm_mask = np.where(aparc_roi != 0)\n",
    "            print('target: ', ftarget)\n",
    "            print('loss: ', floss)\n",
    "            print('a,b,c: ', a,b,c)\n",
    "\n",
    "            # apply optimization to each hemisphere\n",
    "            for gm_mask, hem, start_location, good_coords, good_coords_V1, good_coords_V2, good_coords_V3, dim2 in zip([GM_LH, GM_RH], ['LH', 'RH'], [median_lh, median_rh], [good_coords_lh, good_coords_rh], [V1_coords_lh, V1_coords_rh], [V2_coords_lh, V2_coords_rh], [V3_coords_lh, V3_coords_rh], [dim2_lh, dim2_rh]):        \n",
    "                \n",
    "                # check if already done\n",
    "                data_id = str(s) + '_' + str(hem) + '_V1_n1000_1x10_' + floss + '_' + str(thresh) + '_' + ftarget                \n",
    "                fname = '/mnt/c/DATA/NESTOR/HCP/simresults/' + data_id + 'it' + '.pkl'              \n",
    "                if os.path.exists(fname):\n",
    "                    print(str(s), ' ', str(hem), ' ',  str(ftarget), ' ', str(floss), ' already processed.')\n",
    "                else:\n",
    "                    dimensions = [dim1, dim2, dim3, dim4]\n",
    "\n",
    "                    # create initial point generator\n",
    "                    lhs2 = cook_initial_point_generator(\"lhs\", criterion=\"maximin\")\n",
    "\n",
    "                    # optimize\n",
    "                    res = gp_minimize(f, x0=x0, dimensions=dimensions, n_jobs=-1, n_calls=num_calls, n_initial_points=num_initial_points, initial_point_generator=lhs2, callback=[custom_stopper])\n",
    "\n",
    "                    # print results\n",
    "                    print('subject ', s, ' ', hem)\n",
    "                    print('best alpha:', res.x[0])\n",
    "                    print('best beta:',res.x[1])\n",
    "                    print('best offset_from_base:', res.x[2])\n",
    "                    print('best shank_length:',res.x[3])\n",
    "                    grid_valid, dice, hell_d, grid_yield, phosphenes, phosphenes_V1, phosphenes_V2, phosphenes_V3, contacts_xyz_moved, phospheneMap = f_manual(res.x[0], res.x[1],res.x[2], res.x[3], good_coords, good_coords_V1, good_coords_V2, good_coords_V3, target_density)\n",
    "                    print('best dice, yield, KL: ', dice, grid_yield, hell_d)\n",
    "\n",
    "                    # show resulting binary phosphene map (reflects dice coefficient)\n",
    "                    bin_thresh=np.percentile(phospheneMap, dc_percentile) #np.min(target_density) # bin_thesh determines size target\n",
    "                    phospheneMap_bin = (phospheneMap > bin_thresh).astype(bool)\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "                    fig.suptitle('binarized vs. raw phospheneMap')\n",
    "                    plt.subplot(1,2,1).imshow(phospheneMap_bin, cmap = 'seismic', vmin=0, vmax=np.max(phospheneMap)/100)\n",
    "                    plt.subplot(1,2,2).imshow(phospheneMap, cmap = 'seismic', vmin=0, vmax=np.max(phospheneMap)/100)\n",
    "                    plt.show()\n",
    "                    print('    max phospheneMap: ', np.max(phospheneMap))        \n",
    "\n",
    "                    # Saving the objects\n",
    "                    data_id = str(s) + '_' + str(hem) + '_V1_n1000_1x10_' + floss + '_' + str(thresh) + '_' + ftarget                    \n",
    "                    fname = outputfolder + data_id + '.pkl'\n",
    "                    with open(fname, 'wb') as file:\n",
    "                        pickle.dump([res, \n",
    "                                     grid_valid, \n",
    "                                     dice, hell_d, \n",
    "                                     grid_yield, \n",
    "                                     contacts_xyz_moved,\n",
    "                                     good_coords,\n",
    "                                     good_coords_V1,\n",
    "                                     good_coords_V2,\n",
    "                                     good_coords_V3,\n",
    "                                     phosphenes,\n",
    "                                     phosphenes_V1,\n",
    "                                     phosphenes_V2,\n",
    "                                     phosphenes_V3], file, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
